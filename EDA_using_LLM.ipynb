{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iammawaistariq/PAI-Lab/blob/main/EDA_using_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA using LLM**\n",
        "* ChatGPT LLM (PandasLLM)\n",
        "* Gemini LLM  \n"
      ],
      "metadata": {
        "id": "wyI97r2ai6eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pandas pandas-llm google-generativeai"
      ],
      "metadata": {
        "id": "C0DLNiRHDB1w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYKBXn5esk2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "personal_GPT_API = userdata.get('personal_GPT_API')\n",
        "personal_Gemini = userdata.get('personal_Gemini')\n",
        "Gemini_API_key2 = \"AIzaSyAC2JlhVLo6-9XXSXwZBwirPzmkd6y2ZxI\""
      ],
      "metadata": {
        "id": "bsmTjnTms7sL"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjillBrYskna"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pandas_llm import PandasLLM\n",
        "\n",
        "# Data\n",
        "# Please note that these names, ages, and donations are randomly generated\n",
        "# and do not correspond to real individuals or their donations.\n",
        "data = [('John Doe', 25, 50),\n",
        "        ('Jane Smith', 38, 70),\n",
        "        ('Alex Johnson', 45, 80),\n",
        "        ('Jessica Brown', 60, 40),\n",
        "        ('Michael Davis', 22, 90),\n",
        "        ('Emily Wilson', 30, 60),\n",
        "        ('Daniel Taylor', 35, 75),\n",
        "        ('Sophia Moore', 40, 85),\n",
        "        ('David Thomas', 50, 65),\n",
        "        ('Olivia Jackson', 29, 55)]\n",
        "df = pd.DataFrame(data, columns=['name', 'age', 'donation'])\n",
        "\n",
        "api_key = personal_GPT_API\n",
        "\n",
        "conv_df = PandasLLM(data=df, llm_api_key = api_key)\n",
        "result = conv_df.prompt(\"What is the average donation of people older than 40 who donated more than $50?\")\n",
        "code = conv_df.code_block\n",
        "\n",
        "print(f\"Executing the following expression of type {type(result)}:\\n{code}\\n\\nResult is:\\n {result}\\n\")\n",
        "# Executing the following expression of type <class 'numpy.float64'>:\n",
        "# result = df.loc[(df['age'] > 40) & (df['donation'] > 50), 'donation'].mean()\n",
        "\n",
        "# Result is:\n",
        "#  72.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzJHvIpe02aj",
        "outputId": "f1c75f5d-2fd1-47ce-86eb-4b7dcc82b9ab"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing the following expression of type <class 'str'>:\n",
            "\n",
            "\n",
            "Result is:\n",
            " Please try later\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-a50fc9c09d0c>:22: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  conv_df = PandasLLM(data=df, llm_api_key = api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "\n",
        "# Get your Gemini API key\n",
        "genai.configure(api_key=personal_Gemini)\n",
        "\n",
        "# Read Dataset into DataFrame\n",
        "titanic = pd.read_csv(\"/content/titanic.csv\")\n",
        "\n",
        "# Define function to ask Gemini LLM\n",
        "def ask_gemini(prompt, titanic):\n",
        "    system_prompt = f\"\"\"You are a Python pandas expert.\n",
        "You are given a pandas DataFrame named `titanic`. Write Python code that answers the question below.\n",
        "\n",
        "Question: {prompt}\n",
        "\"\"\"\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-pro-001\")  # Use valid model name\n",
        "    response = model.generate_content(system_prompt)\n",
        "    print(response)\n",
        "    code = response.text.strip(\"```python\").strip(\"```\")\n",
        "    return code\n"
      ],
      "metadata": {
        "id": "FjEHvf-oaCYY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask your question\n",
        "question = \"How many missing values are there in the age column?\"\n",
        "code = ask_gemini(question, titanic)\n",
        "\n",
        "print(\"ðŸ”§ Generated code:\\n\")\n",
        "print(code)\n"
      ],
      "metadata": {
        "id": "D6mHqKbgtHH_",
        "outputId": "3e15d0eb-356d-4896-d5db-85eda24b32b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"```python\\ntitanic['Age'].isnull().sum()\\n```\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            }\n",
            "          ]\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 43,\n",
            "        \"candidates_token_count\": 13,\n",
            "        \"total_token_count\": 56\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-pro-001\"\n",
            "    }),\n",
            ")\n",
            "ðŸ”§ Generated code:\n",
            "\n",
            "\n",
            "titanic['Age'].isnull().sum()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic['Age'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP8MnGj_1x6u",
        "outputId": "7327083f-9b4a-4b39-ae44-e3457e14b0b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(177)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "\n",
        "# Get your Gemini API key\n",
        "genai.configure(api_key=personal_Gemini)\n",
        "\n",
        "# Read Dataset into DataFrame\n",
        "titanic = pd.read_csv(\"/content/titanic.csv\")\n",
        "\n",
        "# Define function to ask Gemini LLM\n",
        "def ask_gemini(prompt, titanic):\n",
        "    system_prompt = f\"\"\"You are a Python pandas expert.\n",
        "You are given a pandas DataFrame named `titanic`. Write Python code that answers the question below.\n",
        "Assign the output to a variable called `result1`. Only return valid Python code.\n",
        "\n",
        "Question: {prompt}\n",
        "\"\"\"\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-pro-001\")  # Use valid model name\n",
        "    response = model.generate_content(system_prompt)\n",
        "    code = response.text.strip(\"```python\").strip(\"```\")\n",
        "    return code\n",
        "\n",
        "# Ask your question\n",
        "question = \"How many distinct ages are there in the dataset?\"\n",
        "code = ask_gemini(question, titanic)\n",
        "\n",
        "print(\"ðŸ”§ Generated code:\\n\")\n",
        "print(code)\n",
        "\n",
        "# Execute the code\n",
        "local_vars = {\"titanic\": titanic}\n",
        "#print(local_vars)\n",
        "exec(code, local_vars)\n",
        "if \"result1\" in local_vars:\n",
        "  print(local_vars[\"result1\"])\n",
        "\n",
        "# Show result if available\n",
        "#if \"result\" in local_vars:\n",
        "    #from IPython.display import display\n",
        "    #display(local_vars[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "7hGiUxY3d1lC",
        "outputId": "0b921ed2-7e86-4889-abd1-5da592d9c1f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Generated code:\n",
            "\n",
            "\n",
            "result1 = titanic[\"Age\"].nunique()\n",
            "\n",
            "88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Task\n",
        "\n",
        "* Get your OpenAI and Gemini API keys, and execute the above code.\n",
        "* Modify the prompt and analyze the LLM's responses to various prompts.\n",
        "* Research the different versions of ChatGPT and Gemini LLMs available, and determine which version performs best for the EDA process.\n",
        "* Exlpore the PandasAI library (https://pypi.org/project/pandasai/) and how it is different from PandasLLM.  \n",
        "* Apply EDA using an LLM on the following dataset.\n",
        "\n",
        "https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ],
      "metadata": {
        "id": "Vu7nZz_DrFFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = pd.read_csv(\"/content/titanic.csv\")\n"
      ],
      "metadata": {
        "id": "12RzZRIWtS3-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Function"
      ],
      "metadata": {
        "id": "V0Ykeyghp466"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "def Ask_Gemini(question, dataset=titanic):\n",
        "    # genai.configure(api_key=personal_Gemini)\n",
        "    genai.configure(api_key=Gemini_API_key2)\n",
        "\n",
        "    system_prompt = f\"\"\"You are a dataset expert. You will be given a dataset and question related to that dataset.\n",
        "    Not interested in code or useless explanations, Your job is to give accurate answer for the questions.\n",
        "    Your resposne should be in meaingful sentences.\n",
        "\n",
        "    Dataset: {dataset}\n",
        "\n",
        "    Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-pro-001\")\n",
        "    response = model.generate_content(system_prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "IIi6SXOptSwI"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Function"
      ],
      "metadata": {
        "id": "-zj9R3_uqKCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def Ask_GPT(question, dataset=titanic):\n",
        "\n",
        "    client = OpenAI(api_key=personal_GPT_API)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are a dataset expert. You will be given a dataset and question related to that dataset. Not interested in code or useless explanations, Your job is to give accurate answer for the questions. Your resposne should be in meaingful sentences.\n",
        "\n",
        "                Dataset: {dataset}\n",
        "                \"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"User's question = {question}\"\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "sojQxohM5l7L"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions"
      ],
      "metadata": {
        "id": "PsoxikSmqMTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1"
      ],
      "metadata": {
        "id": "HRUoYL1svpXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_1 = \"How many female survived and how many died?\"\n",
        "\n",
        "gemeni_response_question_1 = Ask_Gemini(question_1)\n",
        "\n",
        "GPT_response_question_1 = Ask_GPT(question_1)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {question_1}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemeni_response_question_1}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_response_question_1}\")\n",
        "print(\"=\"*80)\n",
        "print(\"Actual Values \\n\")\n",
        "\n",
        "####################################################################################################\n",
        "survied_female = titanic[(titanic['Sex']=='female') & (titanic['Survived']==1)].shape[0]\n",
        "print(f\"Total {survied_female} female survived\")\n",
        "\n",
        "not_survied_female = titanic[(titanic['Sex']=='female') & (titanic['Survived']==0)].shape[0]\n",
        "print(f\"Total {not_survied_female} female survived\")\n",
        "####################################################################################################\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "dMpksmlhtSiM",
        "outputId": "b55ab478-0e0b-4dab-af0c-083988eee980"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = How many female survived and how many died?\n",
            "================================================================================\n",
            "Gemini Response = 344 female passengers survived and 81 female passengers died. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = The dataset contains information of 314 female passengers. Out of these, 233 females survived and 81 died.\n",
            "================================================================================\n",
            "Actual Values \n",
            "\n",
            "Total 233 female survived\n",
            "Total 81 female survived\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fe3lEG3NLmUc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2K4i7hWHvDga"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2"
      ],
      "metadata": {
        "id": "DKXVLhD_vnV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_2 = \"How many missing values in each column and how many total rows with missing values?\"\n",
        "\n",
        "gemeni_response_question_2 = Ask_Gemini(question_2)\n",
        "\n",
        "GPT_response_question_2 = Ask_GPT(question_2)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {question_2}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemeni_response_question_2}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_response_question_2}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Actual Values \\n {titanic.isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "W2K2aGrtulw1",
        "outputId": "22c91881-3fc6-4073-8924-8f40caa6ecec"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = How many missing values in each column and how many total rows with missing values?\n",
            "================================================================================\n",
            "Gemini Response = The 'Cabin' column has 687 missing values, 'Age' has 177, 'Embarked' has 2, and 'AgeGroup' also has 177.  There are a total of 708 rows with at least one missing value. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = There are missing values in the 'Age', 'Cabin' and 'Embarked' columns. Specifically, the 'Age' column has 177 missing values, the 'Cabin' column has 687 missing values, and the 'Embarked' column has 2 missing values summing to a total of 866 rows with missing values.\n",
            "================================================================================\n",
            "Actual Values \n",
            " PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "AgeGroup       177\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4XfYuzBPuluk"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OdbmZog8ulsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3"
      ],
      "metadata": {
        "id": "8L9Qmr_pwMwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_3 = \"What was average far for the survived and not-survied, and what was highest fare for the not-survived and lowest fare for survievd?\"\n",
        "\n",
        "\n",
        "question_2 = \"How many missing values in each column and how many total rows with missing values?\"\n",
        "\n",
        "gemeni_response_question_3 = Ask_Gemini(question_3)\n",
        "\n",
        "GPT_response_question_3 = Ask_GPT(question_3)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {question_3}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemeni_response_question_3}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_response_question_3}\")\n",
        "print(\"=\"*80)\n",
        "print(\"Actual Values \\n\")\n",
        "\n",
        "fare_survived = titanic[titanic['Survived'] == 1]['Fare']\n",
        "\n",
        "average_fare_survived = fare_survived.mean()\n",
        "print(f\"Average fare for survived passengers: {average_fare_survived}\")\n",
        "\n",
        "lowest_fare_survived = fare_survived.min()\n",
        "print(f\"Lowest fare for survived passengers: {lowest_fare_survived}\")\n",
        "\n",
        "###################################################################\n",
        "fare_not_survived = titanic[titanic['Survived'] == 0]['Fare']\n",
        "\n",
        "average_fare_not_survived = fare_not_survived.mean()\n",
        "print(f\"Average fare for not-survived passengers: {average_fare_not_survived}\")\n",
        "\n",
        "highest_fare_not_survived = fare_not_survived.max()\n",
        "print(f\"Highest fare for not-survived passengers: {highest_fare_not_survived}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "PFm0sVoyulp2",
        "outputId": "a8f42ec1-98d0-44eb-cc52-c3c8cb46e92d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = What was average far for the survived and not-survied, and what was highest fare for the not-survived and lowest fare for survievd?\n",
            "================================================================================\n",
            "Gemini Response = The average fare for passengers who survived was higher, at approximately $48.40, compared to the average fare for those who did not survive, which was about $22.12.  The highest fare paid by a passenger who did not survive the disaster was $263, while the lowest fare paid by a surviving passenger was $0. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = The average fare for the survived passengers was approximately $48.40, while for the not-survived, it was around $22.12. The highest fare for the not-survived was $263.00, and the lowest fare for the survived was $0.00.\n",
            "================================================================================\n",
            "Actual Values \n",
            "\n",
            "Average fare for survived passengers: 48.39540760233918\n",
            "Lowest fare for survived passengers: 0.0\n",
            "Average fare for not-survived passengers: 22.117886885245902\n",
            "Highest fare for not-survived passengers: 263.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKhr7bXyulna"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_4 = \"Among children under 12 years, compare survival rates between males and females traveling alone versus with family. Which group was the safest?\"\n",
        "\n",
        "gemeni_response_question_4 = Ask_Gemini(question_4)\n",
        "\n",
        "GPT_response_question_4 = Ask_GPT(question_4)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {question_4}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemeni_response_question_4}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_response_question_4}\")\n",
        "print(\"=\"*80)\n",
        "print(\"Actual Values \\n\")\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "children = titanic[titanic['Age'] < 12].copy()\n",
        "\n",
        "# Add Alone/With Family status\n",
        "children['FamilyStatus'] = children.apply(lambda row: 'Alone' if (row['SibSp'] == 0 and row['Parch'] == 0) else 'WithFamily', axis=1)\n",
        "\n",
        "grouped = children.groupby(['Sex', 'FamilyStatus'])['Survived'].mean().reset_index()\n",
        "\n",
        "print(\"\\nSurvival Rates among Children (<12) by Gender and Family Status:\")\n",
        "print(grouped)\n"
      ],
      "metadata": {
        "id": "RCRjI98pullG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Akzi63ywulii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heart dataset"
      ],
      "metadata": {
        "id": "xI03UxGVnhg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heart_dataset = pd.read_csv('/content/heart_2020_cleaned.csv')"
      ],
      "metadata": {
        "id": "MKO_SGSmnqCJ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_question_1 = \"What are key factors for heart disease?\"\n",
        "\n",
        "gemini_heart_response_question_1 = Ask_Gemini(heart_question_1, dataset=heart_dataset)\n",
        "\n",
        "GPT_heart_response_question_1 = Ask_GPT(heart_question_1, dataset=heart_dataset)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {heart_question_1}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemini_heart_response_question_1}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_heart_response_question_1}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "yfHVUMJUulgK",
        "outputId": "6414864f-f83e-4302-9f60-b3db494c3ce2"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = What are key factors for heart disease?\n",
            "================================================================================\n",
            "Gemini Response = While this dataset alone can't determine definitive causation, it suggests potential key factors for heart disease risk include smoking, diabetes, physical inactivity, poor general health, and a history of stroke. Additionally, higher BMI and older age categories appear to correlate with a higher heart disease prevalence in this dataset. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = The key factors for heart disease, as per the given dataset, are smoking, BMI (Body Mass Index), alcohol drinking, physical health, mental health, physical activity, sleep time, and potentially other medical conditions such as asthma and kidney disease.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiUZWM_puld1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_question_2 = \"Which features have higher correlation with the heart disease? Give me their correlation vales also.\"\n",
        "\n",
        "gemini_heart_response_question_2 = Ask_Gemini(heart_question_2, dataset=heart_dataset)\n",
        "\n",
        "GPT_heart_response_question_2 = Ask_GPT(heart_question_2, dataset=heart_dataset)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {heart_question_2}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemini_heart_response_question_2}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_heart_response_question_2}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "MKCfeNM8qtnV",
        "outputId": "612df5a0-b9c3-47cd-9e1f-b51f0d563781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = Which features have higher correlation with the heart disease? Give me their correlation vales also.\n",
            "================================================================================\n",
            "Gemini Response = The features that have higher correlation with heart disease are: \n",
            "\n",
            "* **AgeCategory:** as age increases, the risk of heart disease also increases. (Though correlation value is not given, it is a known risk factor)\n",
            "* **PhysicalHealth:**  Deterioration in physical health shows a strong correlation with heart disease. (Again correlation value is not given)\n",
            "* **Diabetic:**  People with diabetes have significantly higher chances of having heart disease. (correlation value is not available in the provided data). \n",
            "\n",
            "Please note that the dataset doesn't contain correlation values between features. You would need to calculate those separately. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = The features with higher correlation with heart disease are BMI, Smoking, Alcohol Drinking, Stroke, Physical Health, Mental Health, Diabetic, Physical Activity, GenHealth, and Asthma. The correlation values for these features range from -0.15 to 0.25.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zM4XXGpHqtkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_question_3 = \"What is ratio of male to female heart disease?\"\n",
        "\n",
        "gemini_heart_response_question_3 = Ask_Gemini(heart_question_3, dataset=heart_dataset)\n",
        "\n",
        "GPT_heart_response_question_3 = Ask_GPT(heart_question_3, dataset=heart_dataset)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {heart_question_3}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemini_heart_response_question_3}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_heart_response_question_3}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "nbYNJd1srFbJ",
        "outputId": "5370e3df-9f06-456d-d93e-933d707afa5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = What is ratio of male to female heart disease?\n",
            "================================================================================\n",
            "Gemini Response = There is no information on the ratio of male to female heart disease sufferers in this dataset. We can only see the breakdown of each gender with heart disease separately. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = The ratio of male to female heart disease in the given dataset is 1.2:1. This indicates that for every 1 female with heart disease, there are approximately 1.2 males with heart disease.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdXRGJOsrFYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_question_4 = \"What is ratio of male to female heart disease?\"\n",
        "\n",
        "gemini_heart_response_question_4 = Ask_Gemini(heart_question_4, dataset=heart_dataset)\n",
        "\n",
        "GPT_heart_response_question_4 = Ask_GPT(heart_question_4, dataset=heart_dataset)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {heart_question_4}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemini_heart_response_question_4}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_heart_response_question_4}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "FAWUUh1evyrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aT0sxYOKvypY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heart_question_5 = \"What is average BMI of patients with heart disease and without heart disease?\"\n",
        "\n",
        "gemini_heart_response_question_5 = Ask_Gemini(heart_question_5, dataset=heart_dataset)\n",
        "\n",
        "GPT_heart_response_question_5 = Ask_GPT(heart_question_5, dataset=heart_dataset)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {heart_question_5}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemini_heart_response_question_5}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_heart_response_question_5}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "wLMT2JXkvyl1",
        "outputId": "cc78270b-d18e-434f-a97f-26ceeea506b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = What is average BMI of patients with heart disease and without heart disease?\n",
            "================================================================================\n",
            "Gemini Response = The average BMI of patients with heart disease is 28.44 and the average BMI of patients without heart disease is 28.32. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = The average BMI of patients without heart disease is 27.38, while the average BMI of patients with heart disease is 28.88.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bmi_with_disease = heart_dataset[heart_dataset['HeartDisease']=='Yes']['BMI'].mean()\n",
        "avg_bmi_without_disease = heart_dataset[heart_dataset['HeartDisease']=='No']['BMI'].mean()\n",
        "\n",
        "print(\"Average BMI of patients with heart disease\", avg_bmi_with_disease)\n",
        "print(\"Average BMI of patients without heart disease\", avg_bmi_without_disease)"
      ],
      "metadata": {
        "id": "IjLTE1RdzJt3",
        "outputId": "aa6fe186-033d-4bf5-ef8d-8ff91958c2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BMI of patients with heart disease 29.401592079786653\n",
            "Average BMI of patients without heart disease 28.224658336240093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heart_question_6 = \"Among non-smokers who do physical activity (PhysicalActivity == 1), what is the combined impact of BMI, SleepTime, and AgeCategory on Heart Disease risk? Build a logistic regression model and find the odds ratios for each factor.?\"\n",
        "\n",
        "gemini_heart_response_question_6 = Ask_Gemini(heart_question_6, dataset=heart_dataset)\n",
        "\n",
        "GPT_heart_response_question_6 = Ask_GPT(heart_question_6, dataset=heart_dataset)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Question = {heart_question_6}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Gemini Response = {gemini_heart_response_question_6}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPT Response = {GPT_heart_response_question_6}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "tp69NiybzUfM",
        "outputId": "22d6daa8-1fb6-4522-e2da-2de197b009e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Question = Among non-smokers who do physical activity (PhysicalActivity == 1), what is the combined impact of BMI, SleepTime, and AgeCategory on Heart Disease risk? Build a logistic regression model and find the odds ratios for each factor.?\n",
            "================================================================================\n",
            "Gemini Response = I cannot provide you with an exact odds ratio for BMI, SleepTime, and AgeCategory without performing calculations on the provided data. \n",
            "\n",
            "However, I can guide you on how to interpret the results of a logistic regression model that you would build using a statistical software: \n",
            "\n",
            "1. **Odds Ratios:** After building the model, each of the factors (BMI, SleepTime, AgeCategory) will have an associated odds ratio. \n",
            "    * An odds ratio greater than 1 indicates that an increase in the variable is associated with *higher* odds of heart disease within the non-smoking, physically active group.\n",
            "    * An odds ratio less than 1 means an increase in the variable is associated with *lower* odds of heart disease. \n",
            "    * An odds ratio close to 1 suggests that the variable doesn't have a strong impact on the odds of heart disease.\n",
            "\n",
            "2. **Age Category:**  Since AgeCategory is not a continuous variable, you'll get separate odds ratios for each age group compared to a reference age group (which the software will choose by default).\n",
            "\n",
            "**Remember:** Correlation does not equal causation. Even if the odds ratios show associations, it doesn't necessarily mean these factors directly cause or prevent heart disease. \n",
            "\n",
            "================================================================================\n",
            "GPT Response = I am sorry. I can't assist with that.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: Filter\n",
        "filtered = heart_dataset[(heart_dataset['Smoking'] == 0) & (heart_dataset['PhysicalActivity'] == 1)]\n",
        "\n",
        "# Step 2: Prepare variables\n",
        "# Select needed columns\n",
        "X = filtered[['BMI', 'SleepTime', 'AgeCategory']]\n",
        "y = filtered['HeartDisease']\n",
        "\n",
        "# One-hot encode AgeCategory\n",
        "X = pd.get_dummies(X, columns=['AgeCategory'], drop_first=True)\n",
        "\n",
        "# Add constant for intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Step 3: Logistic Regression\n",
        "model = sm.Logit(y, X)\n",
        "result = model.fit()\n",
        "\n",
        "# Step 4: Show coefficients and Odds Ratios\n",
        "print(result.summary())\n",
        "\n",
        "# Calculate Odds Ratios\n",
        "odds_ratios = np.exp(result.params)\n",
        "print(\"\\nOdds Ratios:\")\n",
        "print(odds_ratios)\n"
      ],
      "metadata": {
        "id": "s4fR29lm49pl",
        "outputId": "557707cc-db57-4452-9dc2-4f931d507e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-4e8db7db0b3e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Step 3: Logistic Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, offset, check_rank, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# unconditional check, requires no extra kwargs added by subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         super().__init__(endog, exog, offset=offset, check_rank=check_rank,\n\u001b[0m\u001b[1;32m    476\u001b[0m                          **kwargs)\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, check_rank, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_on_perfect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# keep for backwards compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_extra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0m\u001b[1;32m     96\u001b[0m                                       **kwargs)\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0m\u001b[1;32m    676\u001b[0m                  **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             raise ValueError(\"Pandas data cast to numpy dtype of object. \"\n\u001b[0m\u001b[1;32m    510\u001b[0m                              \"Check input data with np.asarray(data).\")\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DcT1Z1f-5JrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}